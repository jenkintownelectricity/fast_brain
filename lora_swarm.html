<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The LoRA Swarm: Infinite Intelligence</title>
    <style>
        :root {
            --bg-dark: #0a0a0f;
            --card-bg: #14141f;
            --accent-cyan: #00f2ff;
            --accent-magenta: #ff0055;
            --accent-green: #00ff9d;
            --accent-gold: #ffb86c;
            --text-main: #e0e0e0;
            --text-muted: #858595;
            --font-stack: 'Inter', system-ui, -apple-system, sans-serif;
            --code-font: 'Fira Code', monospace;
        }

        body {
            background-color: var(--bg-dark);
            color: var(--text-main);
            font-family: var(--font-stack);
            margin: 0;
            padding: 0;
            line-height: 1.6;
            overflow-x: hidden;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 4rem 2rem;
        }

        /* --- HERO SECTION --- */
        header {
            text-align: center;
            margin-bottom: 6rem;
            position: relative;
        }

        h1 {
            font-size: 4rem;
            font-weight: 800;
            margin: 0;
            background: linear-gradient(135deg, #fff 0%, #a5a5a5 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            letter-spacing: -0.05em;
            text-transform: uppercase;
        }

        .subtitle {
            font-size: 1.5rem;
            color: var(--accent-cyan);
            margin-top: 1rem;
            font-weight: 300;
            letter-spacing: 0.1em;
        }

        /* --- VISUAL DIAGRAMS --- */
        .diagram-container {
            background: var(--card-bg);
            border: 1px solid rgba(255,255,255,0.05);
            border-radius: 24px;
            padding: 3rem;
            margin-bottom: 4rem;
            position: relative;
            overflow: hidden;
        }

        /* 1. THE CORE CONCEPT */
        .core-concept {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 4rem;
            padding: 2rem 0;
        }

        .frozen-brain {
            width: 180px;
            height: 180px;
            background: radial-gradient(circle at 30% 30%, #2a2a35, #000);
            border: 2px solid var(--text-muted);
            border-radius: 50%;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            box-shadow: 0 0 30px rgba(0,0,0,0.5);
            position: relative;
            z-index: 10;
        }

        .orbit-track {
            position: absolute;
            width: 400px;
            height: 400px;
            border: 1px dashed rgba(255,255,255,0.1);
            border-radius: 50%;
            animation: spin 20s linear infinite;
        }

        .lora-node {
            position: absolute;
            width: 60px;
            height: 60px;
            background: rgba(20, 20, 31, 0.9);
            border: 2px solid var(--accent-cyan);
            border-radius: 12px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5rem;
            box-shadow: 0 0 15px rgba(0, 242, 255, 0.2);
            transition: all 0.3s ease;
        }

        /* Positioning nodes on the orbit */
        .lora-node:nth-child(1) { top: -30px; left: 50%; transform: translateX(-50%); } /* 12 o'clock */
        .lora-node:nth-child(2) { bottom: -30px; left: 50%; transform: translateX(-50%); border-color: var(--accent-magenta); } /* 6 o'clock */
        .lora-node:nth-child(3) { top: 50%; left: -30px; transform: translateY(-50%); border-color: var(--accent-green); } /* 9 o'clock */
        .lora-node:nth-child(4) { top: 50%; right: -30px; transform: translateY(-50%); border-color: var(--accent-gold); } /* 3 o'clock */

        @keyframes spin { 100% { transform: rotate(360deg); } }

        /* --- THE PROCESS FLOW --- */
        .process-flow {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 1.5rem;
            margin-top: 2rem;
        }

        .step-card {
            background: rgba(255,255,255,0.03);
            border-radius: 16px;
            padding: 2rem;
            position: relative;
            border-top: 4px solid #333;
            transition: transform 0.3s ease;
        }

        .step-card:hover {
            transform: translateY(-10px);
            background: rgba(255,255,255,0.05);
        }

        .step-card.ingest { border-color: var(--accent-cyan); }
        .step-card.train { border-color: var(--accent-magenta); }
        .step-card.store { border-color: var(--accent-gold); }
        .step-card.deploy { border-color: var(--accent-green); }

        .step-icon {
            font-size: 2.5rem;
            margin-bottom: 1rem;
            display: block;
        }

        .step-title {
            font-size: 1.2rem;
            font-weight: 700;
            margin-bottom: 0.5rem;
            color: #fff;
        }

        /* --- CODE SECTION --- */
        .code-window {
            background: #000;
            border-radius: 12px;
            border: 1px solid rgba(255,255,255,0.1);
            font-family: var(--code-font);
            font-size: 0.9rem;
            margin-top: 2rem;
            box-shadow: 0 10px 40px rgba(0,0,0,0.5);
        }

        .window-header {
            background: #1a1a25;
            padding: 0.8rem 1.5rem;
            border-bottom: 1px solid rgba(255,255,255,0.1);
            display: flex;
            gap: 0.5rem;
            border-radius: 12px 12px 0 0;
        }

        .dot { width: 12px; height: 12px; border-radius: 50%; }
        .red { background: #ff5f56; }
        .yellow { background: #ffbd2e; }
        .green { background: #27c93f; }

        .code-content {
            padding: 1.5rem;
            color: #a5b3ce;
            white-space: pre-wrap;
        }

        .kw { color: #c678dd; } /* keyword */
        .func { color: #61afef; } /* function */
        .str { color: #98c379; } /* string */
        .num { color: #d19a66; } /* number */
        .comment { color: #5c6370; font-style: italic; }

        /* --- EXPLANATION TEXT --- */
        .explainer {
            margin-top: 4rem;
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 4rem;
        }

        .explainer h3 {
            font-size: 2rem;
            margin-bottom: 1.5rem;
            color: #fff;
        }

        .explainer p {
            color: var(--text-muted);
            margin-bottom: 1.5rem;
            font-size: 1.1rem;
        }

        .highlight-box {
            background: rgba(0, 242, 255, 0.05);
            border-left: 4px solid var(--accent-cyan);
            padding: 1.5rem;
            margin: 1.5rem 0;
            color: var(--text-main);
        }

    </style>
</head>
<body>

<header>
    <h1>The LoRA Swarm</h1>
    <div class="subtitle">Infinite Intelligence on a Budget</div>
</header>

<div class="container">

    <!-- VISUAL 1: THE HIVE MIND -->
    <div class="diagram-container">
        <div style="position: absolute; top: 20px; left: 20px; font-weight: bold; color: var(--text-muted);">
            FIGURE 1: THE ARCHITECTURE
        </div>
        
        <div class="core-concept">
            <!-- The Base Brain -->
            <div class="frozen-brain">
                <span style="font-size: 3rem;">üßä</span>
                <strong style="margin-top: 10px;">BitNet LPU</strong>
                <span style="font-size: 0.8rem; color: var(--text-muted);">Base Model (Frozen)</span>
            </div>

            <!-- The Orbiting Skills -->
            <div class="orbit-track">
                <div class="lora-node" title="Electrical Code">‚ö°</div>
                <div class="lora-node" title="Legal/Compliance">‚öñÔ∏è</div>
                <div class="lora-node" title="Scheduling/Calendar">üìÖ</div>
                <div class="lora-node" title="Emergency Protocol">üö®</div>
            </div>
        </div>
        
        <div style="text-align: center; margin-top: 2rem; color: var(--text-muted);">
            The <strong>Base Model</strong> (Frozen LPU) provides the grammar and reasoning.<br>
            The <strong>Swarm</strong> (Orbiting Adapters) provides the specific domain knowledge.
        </div>
    </div>

    <!-- VISUAL 2: THE PIPELINE -->
    <h2 style="text-align: center; font-size: 2.5rem; margin-bottom: 2rem;">The "Expert" Factory Process</h2>
    <div class="process-flow">
        
        <!-- Step 1 -->
        <div class="step-card ingest">
            <span class="step-icon">üìÑ</span>
            <div class="step-title">1. Ingest</div>
            <p style="font-size: 0.9rem; color: var(--text-muted);">
                Feed raw documentation (PDFs, Manuals, Code) into the system.
            </p>
            <div style="margin-top: 1rem; font-size: 0.8rem; color: var(--accent-cyan);">Input: NEC_2025.pdf</div>
        </div>

        <!-- Step 2 -->
        <div class="step-card train">
            <span class="step-icon">‚öôÔ∏è</span>
            <div class="step-title">2. Train (LoRA)</div>
            <p style="font-size: 0.9rem; color: var(--text-muted);">
                Use <strong>Unsloth</strong> to train a tiny "difference matrix" (Adapter) in minutes.
            </p>
            <div style="margin-top: 1rem; font-size: 0.8rem; color: var(--accent-magenta);">Time: ~5 Minutes</div>
        </div>

        <!-- Step 3 -->
        <div class="step-card store">
            <span class="step-icon">üíæ</span>
            <div class="step-title">3. Store</div>
            <p style="font-size: 0.9rem; color: var(--text-muted);">
                Save the adapter as a small file (~10-50MB) in your local "Skill Registry."
            </p>
            <div style="margin-top: 1rem; font-size: 0.8rem; color: var(--accent-gold);">File: electrician.lora</div>
        </div>

        <!-- Step 4 -->
        <div class="step-card deploy">
            <span class="step-icon">üöÄ</span>
            <div class="step-title">4. Hot-Swap</div>
            <p style="font-size: 0.9rem; color: var(--text-muted);">
                The LPU loads the file into RAM instantly when the user asks a relevant question.
            </p>
            <div style="margin-top: 1rem; font-size: 0.8rem; color: var(--accent-green);">Latency: &lt; 10ms</div>
        </div>

    </div>

    <!-- DEEP DIVE SECTION -->
    <div class="explainer">
        <div>
            <h3>Why "LoRA"?</h3>
            <p><strong>LoRA</strong> stands for <em>Low-Rank Adaptation</em>. Imagine trying to learn a new language. You don't rewire your entire brain (Fine-Tuning); you just add a new "dictionary" to your memory.</p>
            <p>Traditional training updates <strong>100%</strong> of the model's weights. LoRA freezes the model and only trains <strong>0.1%</strong> of new parameters.</p>
            
            <div class="highlight-box">
                <strong>The Math Magic:</strong><br>
                Instead of a 100GB matrix update, LoRA decomposes it into two tiny matrices (Rank A and Rank B) that multiply to approximate the change. This is why the files are so small (MBs vs GBs).
            </div>
        </div>

        <div>
            <h3>The Router Pattern</h3>
            <p>How does the Agent know <em>which</em> skill to load?</p>
            <p>You use a tiny, ultra-fast classifier (like a BERT model or even a Keyword Matcher) as the "Router."</p>
            <ul style="list-style: none; padding: 0; color: var(--text-muted);">
                <li style="margin-bottom: 1rem;">
                    <span style="color: var(--accent-cyan); font-weight: bold;">User:</span> "The breaker is buzzing."<br>
                    <span style="color: var(--accent-green);">Router:</span> Detected intent: <code>Electrical_Fault</code>.<br>
                    <span style="color: var(--accent-magenta);">Action:</span> Load <code>electrician.lora</code>.
                </li>
                <li>
                    <span style="color: var(--accent-cyan); font-weight: bold;">User:</span> "Book an appointment."<br>
                    <span style="color: var(--accent-green);">Router:</span> Detected intent: <code>Scheduling</code>.<br>
                    <span style="color: var(--accent-magenta);">Action:</span> Load <code>secretary.lora</code>.
                </li>
            </ul>
        </div>
    </div>

    <!-- CODE SNIPPET -->
    <h3 style="margin-top: 4rem; margin-bottom: 1rem;">The Automation Script</h3>
    <div class="code-window">
        <div class="window-header">
            <div class="dot red"></div>
            <div class="dot yellow"></div>
            <div class="dot green"></div>
            <span style="margin-left: 1rem; color: #fff; font-size: 0.8rem;">train_expert.py</span>
        </div>
        <div class="code-content">
<span class="kw">from</span> unsloth <span class="kw">import</span> FastLanguageModel

<span class="comment"># 1. Load the "Empty" Brain (4-bit quantization for speed)</span>
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = <span class="str">"unsloth/llama-3-8b-Instructbnb-4bit"</span>,
    max_seq_length = <span class="num">2048</span>,
    load_in_4bit = <span class="kw">True</span>
)

<span class="comment"># 2. Attach the LoRA Adapters (The "Cartridge" Slot)</span>
model = FastLanguageModel.get_peft_model(
    model,
    r = <span class="num">16</span>, <span class="comment"># The "Rank" - higher = smarter but slower</span>
    target_modules = [<span class="str">"q_proj"</span>, <span class="str">"k_proj"</span>, <span class="str">"v_proj"</span>, <span class="str">"o_proj"</span>], <span class="comment"># Attention modules</span>
    lora_alpha = <span class="num">16</span>,
    use_gradient_checkpointing = <span class="str">"unsloth"</span>
)

<span class="comment"># 3. Train on your specific Manual/PDF</span>
trainer = SFTTrainer(
    model = model,
    train_dataset = load_dataset(<span class="str">"json"</span>, data_files=<span class="str">"nec_code_2025.jsonl"</span>),
    dataset_text_field = <span class="str">"text"</span>,
    max_steps = <span class="num">60</span> <span class="comment"># Extremely fast training loop</span>
)

trainer.train()

<span class="comment"># 4. Export the Skill</span>
model.save_pretrained(<span class="str">"adapters/electrician_expert"</span>)
print(<span class="str">"‚úÖ Skill Cartridge Created!"</span>)
        </div>
    </div>

</div>

</body>
</html>